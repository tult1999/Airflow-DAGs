import json
import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
#COPY {table} FROM {path} DELIMITER ',' CSV HEADER;
        # COMMIT;
default_args = {
    'start_date': datetime(2023, 5, 21),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def load_data_to_postgres():
    pg_hook = PostgresHook(postgres_conn_id='my_postgres_db')
    file_folder = 'Olist'
    file_name = 'Customers.csv'
    table = 'Customers'
    path = f'/Datafile/Outputfile/{file_folder}/{file_name}'

    query = f"COPY {table} FROM '{path}' DELIMITER ',';"

    pg_hook.run(sql=query, autocommit=True)

def exec_olist_postgres_destination_load_data():
    if 1 != 0:
        load_data_to_postgres()

with DAG('postgres_destination_load_data', schedule_interval='0 0 * * *', default_args=default_args, catchup=False) as dag:
    postgres_destination_load_data = PythonOperator(
        task_id='postgres_destination_load_data',
        python_callable=exec_olist_postgres_destination_load_data
    )

    postgres_destination_load_data
